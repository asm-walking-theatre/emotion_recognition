{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Emotion Recognition(BCS).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lakshita2002/FER_brain_and_cognitive_society/blob/master/Emotion_Recognition(BCS).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-z7SNu-y_vU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import imshow\n",
        "%matplotlib inline\n",
        "import cv2\n",
        "import os\n",
        "import math\n",
        "from mlxtend.image import extract_face_landmarks\n",
        "import gc\n",
        "#Essential Keras Functions\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Input,Dense,Conv2D,MaxPooling2D,Dropout\n",
        "from keras.layers import BatchNormalization, Activation, Flatten\n",
        "from keras.models import Model,load_model\n",
        "from keras.utils import to_categorical\n",
        "from keras.optimizers import SGD\n",
        "from keras import backend as K\n",
        "#Essential sklearn Functions\n",
        "from sklearn import model_selection\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.utils import shuffle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtYG63FmTeX_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0CMveFSZpSq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eye_centers(landmarks):\n",
        "    '''\n",
        "    To find the eye centers(mean of the 6 landmark points around the eye)\n",
        "    '''\n",
        "    #36-41 are landmark points surrounding right eye\n",
        "    point1 = (np.mean(landmarks[36:42,0]),np.mean(landmarks[36:42,1]))\n",
        "    #42-47 are landmark points surrounding left eye\n",
        "    point2 = (np.mean(landmarks[42:48,0]),np.mean(landmarks[42:48,1]))\n",
        "\n",
        "    return point1, point2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHnBfSCcVCsm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_angle(point1, point2):\n",
        "    '''\n",
        "    To find angle in degrees for the given two points\n",
        "    '''\n",
        "    # angle in radians\n",
        "    angle_r = math.atan((point2[1] - point1[1])/(point2[0] - point1[0]))\n",
        "    # angle in degrees\n",
        "    angle_d = math.degrees(angle_r)\n",
        "\n",
        "    return angle_d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlE4AkiBSsy5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rotate_image(image,angle):\n",
        "    '''\n",
        "    Returns a rotated image given the angle(degrees) to be rotated and image\n",
        "    '''\n",
        "    rows,cols = image.shape\n",
        "    #Transformation Matrix(M)\n",
        "    M = cv2.getRotationMatrix2D(((rows - 1)/2.0,(cols - 1)/2.0),angle,1)\n",
        "    rot_img = cv2.warpAffine(image, M, (cols,rows))\n",
        "\n",
        "    return rot_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUlto7nGX-uM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocessing(image_data):\n",
        "\n",
        "    '''\n",
        "    The preprocessing involves rotation of the image and image cropping\n",
        "    Arguments :\n",
        "    image_data -- array of images of shape (m,h,w)\n",
        "    Returns :\n",
        "    array of images after preprocessing\n",
        "    '''\n",
        "    preprocessed_faces = []\n",
        "    for img in image_data:\n",
        "        \n",
        "        #landmark detection\n",
        "        #(returns an array of landmarks of shape (68,2))\n",
        "        landmarks = extract_face_landmarks(img)\n",
        "\n",
        "        #detect eye cnters\n",
        "        p1, p2 = eye_centers(landmarks)\n",
        "\n",
        "        #find angle \n",
        "        angle = find_angle(p1, p2)\n",
        "        \n",
        "        #rotate image\n",
        "        rot_img = rotate_image(img, angle)\n",
        "\n",
        "        #find length 'd'\n",
        "        p1_new, p2_new = eye_centers(extract_face_landmarks(rot_img))\n",
        "        d = cv2.norm(np.array(p1_new) - np.array(p2_new))\n",
        "\n",
        "        #mid point of new eye centers\n",
        "        d_mid = ((p2_new[0]+p1_new[0])/2.0,(p2_new[1]+p1_new[1])/2.0)\n",
        "\n",
        "        #point above line joining eye centers\n",
        "        x_up = d_mid[0]\n",
        "        y_up = d_mid[1] - (0.6*d)\n",
        "\n",
        "        #cropping image\n",
        "        x_start = int(landmarks[0,0])\n",
        "        x_end = int(landmarks[16,0])\n",
        "        y_start = int(y_up)\n",
        "        y_end = int(landmarks[8,1])\n",
        "\n",
        "        crop_img = img[y_start:y_end,x_start:x_end]\n",
        "\n",
        "        #resize the cropped image\n",
        "        face_roi = cv2.resize(crop_img,(48,48))\n",
        "        \n",
        "        preprocessed_faces.append(face_roi)\n",
        "\n",
        "    return np.array(preprocessed_faces)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbERjH0noO9V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalization(imagedata, mean, std_dev):\n",
        "    '''\n",
        "    To apply Histogram equalization and \n",
        "    Z-Square Normalization to the preprocessed images\n",
        "    Arguments :\n",
        "    imagedata -- array of preprocessed images of shape (m,h,w)\n",
        "    mean -- mean of imagedata array\n",
        "    std_dev -- standard deviation of imagedata array\n",
        "    Returns :\n",
        "    array of normalized images of shape (m,48,48)\n",
        "    '''\n",
        "    normalized_images = []\n",
        "    for i in range(imagedata.shape[0]):\n",
        "        #Histogram Equalization\n",
        "        hist_eqv = cv2.equalizeHist(imagedata[i])\n",
        "\n",
        "        #Z-Square normalization\n",
        "        zsq_norm = ((hist_eqv - mean)/std_dev)\n",
        "\n",
        "        #Resize\n",
        "        resized_image = cv2.resize(zsq_norm, (48,48))\n",
        "        normalized_images.append(resized_image)\n",
        "\n",
        "    return np.array(normalized_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLd9iT7zgR8j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_and_preprocess_data(dataset,num_classes,jaffe_dir_path,ck_dir_path):\n",
        "    '''\n",
        "    Loads the dataset given and also preprocesses it\n",
        "    '''\n",
        "    jaffe_data_list = []\n",
        "    jaffe_labels_list = []\n",
        "    ck_data_list = []\n",
        "    ck_labels_list = []\n",
        "\n",
        "    if dataset == 'jaffe' or dataset == 'combined':\n",
        "        \n",
        "        express_code = ['HA','AN','DI','FE','SA','SU','NE']\n",
        "        for img in os.listdir(jaffe_dir_path):\n",
        "\n",
        "            label = img[3:5]\n",
        "            if num_classes == 6 and label == 'NE':\n",
        "                continue                                              \n",
        "            read_img = cv2.imread(jaffe_dir_path+img,\n",
        "                                  cv2.IMREAD_GRAYSCALE)\n",
        "            jaffe_data_list.append(read_img)\n",
        "            jaffe_labels_list.append(express_code.index(label))\n",
        "\n",
        "        jaffe_data = np.array(jaffe_data_list)\n",
        "        jaffe_labels = np.array(jaffe_labels_list)\n",
        "        jaffe_preprocessed_data = preprocessing(jaffe_data)\n",
        "    if dataset == 'ck+' or dataset == 'combined':\n",
        "        \n",
        "        express_code = ['happy','anger','disgust','fear','sadness',\n",
        "                        'surprise','contempt']\n",
        "        for emcode in os.listdir(ck_dir_path):\n",
        "\n",
        "            if num_classes == 6 and emcode == 'contempt':\n",
        "                continue\n",
        "            lst = os.listdir(ck_dir_path + emcode + '/')\n",
        "            lst.sort()\n",
        "            for i in range(2,len(lst),3):\n",
        "                read_img = cv2.imread(ck_dir_path+emcode+'/'+lst[i],\n",
        "                                      cv2.IMREAD_GRAYSCALE)\n",
        "                ck_data_list.append(read_img)\n",
        "                ck_labels_list.append(express_code.index(emcode))\n",
        "\n",
        "        ck_data = np.array(ck_data_list)\n",
        "        ck_labels = np.array(ck_labels_list)\n",
        "        ck_preprocessed_data = preprocessing(ck_data)\n",
        "\n",
        "    print('Data loading and Preprocessing is completed')\n",
        "    if dataset == 'combined':\n",
        "        X = np.concatenate((jaffe_preprocessed_data,ck_preprocessed_data),axis = 0)\n",
        "        Y = np.concatenate((jaffe_labels,ck_labels),axis = 0)\n",
        "        preprocessed_data, labels = shuffle(X,Y)\n",
        "    elif dataset == 'ck+':\n",
        "        preprocessed_data, labels = shuffle(ck_preprocessed_data, ck_labels)\n",
        "    elif dataset == 'jaffe':\n",
        "        preprocessed_data, labels = shuffle(jaffe_preprocessed_data, jaffe_labels)\n",
        "    mean, std_dev = preprocessed_data.mean(),preprocessed_data.std()\n",
        "    normalized_data = normalization(preprocessed_data, mean, std_dev)\n",
        "    X_tmp = normalized_data.reshape(normalized_data.shape + (1,))\n",
        "    Y_tmp = to_categorical(labels)\n",
        "    print(X_tmp.shape)\n",
        "    print(Y_tmp.shape)\n",
        "    print('Normalization and Datareshaping is completed')\n",
        "\n",
        "    return X_tmp,Y_tmp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxLZqeN7ndIN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_aug(X_train, y_train, train_batch_size, X_val = None, y_val = None, val_batch_size = None, cv = 'k_fold'):\n",
        "    '''\n",
        "    To apply data augmentation on training data.\n",
        "    Arguments :\n",
        "    X_train and X_val -- array of training and validation data containing images\n",
        "    y_train and y_val -- labels of training and validation images\n",
        "    train_batch_size and val_batch_size contains the batch size of images\n",
        "    Returns:\n",
        "    Iterator for training and validation batch \n",
        "    '''\n",
        "    #create image data augmenatation generator for training and validation data\n",
        "    train_datagen = ImageDataGenerator(rotation_range = 3,\n",
        "                                       rescale = 1.0,\n",
        "                                       horizontal_flip = True,\n",
        "                                       fill_mode = 'nearest')\n",
        "    #create iterator for training and validation data\n",
        "    train_batch = train_datagen.flow(X_train, y_train, batch_size = train_batch_size)\n",
        "\n",
        "    if cv == 'k_fold':\n",
        "        val_datagen = ImageDataGenerator(rescale=1.0)\n",
        "        val_batch = val_datagen.flow(X_val, y_val ,batch_size = val_batch_size)\n",
        "        return (train_batch, val_batch)\n",
        "\n",
        "    elif cv == 'one_fold':\n",
        "        return train_batch "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfiOZPdEp3SY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def recog_model(input_shape, num_classes, beta):\n",
        "    '''\n",
        "    Create the keras model architecture\n",
        "    Arguments:\n",
        "    input_shape -- The dimensions of the input data(h,w,1)\n",
        "    Returns:\n",
        "    The created model after compilation\n",
        "    '''\n",
        "\n",
        "    # input placeholder as a tensor of input_shape\n",
        "    X_input = Input(input_shape)\n",
        "\n",
        "    # define the keras model\n",
        "    X = Conv2D(48, (5,5), strides = (1,1), padding = 'valid', name = 'conv1')(X_input)\n",
        "    X = BatchNormalization(axis = 3, name = 'bn1')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    X = MaxPooling2D((2,2), strides = (2,2), padding = 'valid', name = 'maxpool1')(X)\n",
        "\n",
        "    X = Conv2D(64, (5,5), strides = (1,1), padding = 'valid', activation = 'relu', name = 'conv2')(X)\n",
        "    X = BatchNormalization(axis = 3, name = 'bn2')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    X = MaxPooling2D((2,2), strides = (2,2), padding = 'valid', name = 'maxpool2')(X)\n",
        "\n",
        "    X = Flatten()(X)\n",
        "    X = Dropout(0.5)(X)\n",
        "    X = Dense(units = num_classes,activation = 'softmax',name = 'out',kernel_initializer = 'glorot_normal')(X)\n",
        "\n",
        "    model = Model(inputs = X_input, outputs = X, name = 'recog_model')\n",
        "    sgd = SGD(learning_rate = 0.001, momentum = beta)\n",
        "\n",
        "    #compile the keras model\n",
        "    model.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Q14WsV78-vY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def k_fold_cv(X_tmp, Y_tmp, dataset, num_classes, model_dir, graph_dir,\n",
        "              n_splits, batch_size, iterations, beta):\n",
        "    '''\n",
        "    \n",
        "    Applies K fold cross validation for given Training data\n",
        "    Saves the header file(.h5) for each fold\n",
        "    Returns the Mean and standard deviation of accuracies\n",
        "    '''\n",
        "  \n",
        "    i = 1\n",
        "    model_accuracies = []\n",
        "    for train_index, val_index in KFold(n_splits = n_splits, shuffle = True, random_state = 0).split(X_tmp, Y_tmp):\n",
        "      \n",
        "        X_train, X_val = X_tmp[train_index], X_tmp[val_index]\n",
        "        X_train = X_train.astype('float32')\n",
        "        X_val = X_val.astype('float32')\n",
        "        Y_train, Y_val = Y_tmp[train_index], Y_tmp[val_index]\n",
        "        \n",
        "        print('Model evaluation : ' + str(i))\n",
        "        \n",
        "        #get the data generators from augmentation function\n",
        "        train_batch, val_batch = data_aug(X_train, Y_train, batch_size, X_val, Y_val, batch_size)\n",
        "        #create a model object\n",
        "        Recog_Model = recog_model(X_tmp.shape[1:], num_classes, beta)\n",
        "        if dataset == 'ck+' or dataset == 'combined':\n",
        "            constant = 3\n",
        "        elif dataset == 'jaffe':\n",
        "            constant = 5\n",
        "        #train the model\n",
        "        history = Recog_Model.fit(train_batch, validation_data = val_batch, epochs = iterations,\n",
        "                                  steps_per_epoch = constant * (X_train.shape[0]//batch_size),\n",
        "                                  validation_steps = constant * (X_val.shape[0]//batch_size), verbose = 1)\n",
        "        \n",
        "        #plotting validation loss and training loss vs epochs\n",
        "        loss_train = history.history['loss']\n",
        "        loss_val = history.history['val_loss']\n",
        "        epochs = range(iterations)\n",
        "        plt.plot(epochs, loss_train, 'g', label = 'training_loss')\n",
        "        plt.plot(epochs, loss_val, 'b', label = 'val_loss')\n",
        "        plt.title('Loss Vs Epochs')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.savefig(graph_dir + f'{dataset}{num_classes}classes({i} of {n_splits}fold)losses.png')\n",
        "        plt.clf()\n",
        "\n",
        "        #plotting validation accuracy and training accuracy vs epochs\n",
        "        accuracy_train = history.history['accuracy']\n",
        "        accuracy_val = history.history['val_accuracy']\n",
        "        plt.plot(epochs, accuracy_train, 'g', label = 'training_accuracy')\n",
        "        plt.plot(epochs, accuracy_val, 'b', label = 'val_accuracy')\n",
        "        plt.title('Accuracy Vs Epochs')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.legend()\n",
        "        plt.savefig(graph_dir + f'{dataset}{num_classes}classes({i} of {n_splits}fold)accuracies.png')\n",
        "        plt.clf()\n",
        "\n",
        "        metrics = Recog_Model.evaluate(X_val, Y_val)\n",
        "        print(f'Accuracy fold {i}: ' + str(metrics[1] * 100))\n",
        "        model_accuracies.append(metrics[1] * 100)\n",
        "        Recog_Model.save(model_dir + f'{dataset}{num_classes}classes({i} of {n_splits}fold).h5')\n",
        "        #Discard the present model\n",
        "        del Recog_Model\n",
        "\n",
        "        gc.collect()\n",
        "        K.clear_session()\n",
        "        i += 1\n",
        "\n",
        "    #calculating mean and standard deviation of accuracies\n",
        "    Mean_Accuracy = np.mean(model_accuracies)\n",
        "    standard_deviation = np.std(model_accuracies)\n",
        "\n",
        "    return Mean_Accuracy,standard_deviation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWQ1u9SbvJWP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_fold_cv(X_tmp, Y_tmp, dataset, num_classes, model_dir, graph_dir,\n",
        "                batch_size, iterations, beta):\n",
        "    '''\n",
        "    Train and saves the model as header file\n",
        "    Arguments :\n",
        "    X_tmp -- normalized and reshaped data\n",
        "    Y_tmp -- labels in one hot encodings\n",
        "    '''\n",
        "    #get the data generator from augmentation function\n",
        "    train_batch = data_aug(X_tmp, Y_tmp, batch_size, cv = 'one_fold')\n",
        "    #create a model object\n",
        "    Recog_Model = recog_model(X_tmp.shape[1:], num_classes, beta)\n",
        "    if dataset == 'ck+' or dataset == 'combined':\n",
        "        constant = 3\n",
        "    elif dataset == 'jaffe':\n",
        "        constant = 5\n",
        "    #train the model\n",
        "    history = Recog_Model.fit(train_batch, epochs = iterations, shuffle = True,\n",
        "                              steps_per_epoch = constant * (X_tmp.shape[0]//batch_size))\n",
        "    #plotting training loss vs epochs\n",
        "    loss_train = history.history['loss']\n",
        "    epochs = range(iterations)\n",
        "    plt.plot(epochs, loss_train, 'g', label = 'training_loss')\n",
        "    plt.title('Loss Vs Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.savefig(graph_dir + f'{dataset}{num_classes}classes(1fold)losses.png')\n",
        "    plt.clf()\n",
        "\n",
        "    #plotting training accuracy vs epochs\n",
        "    accuracy_train = history.history['accuracy']\n",
        "    plt.plot(epochs, accuracy_train, 'g', label = 'training_accuracy')\n",
        "    plt.title('Accuracy Vs Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.savefig(graph_dir + f'{dataset}{num_classes}classes(1fold)accuracies.png')\n",
        "    plt.clf()\n",
        "\n",
        "    metrics = Recog_Model.evaluate(X_tmp, Y_tmp)\n",
        "    print('Accuracy one_folded : ' + str(metrics[1] * 100))\n",
        "    Accuracy = metrics[1] * 100\n",
        "    Recog_Model.save(model_dir + f'{dataset}{num_classes}classes(1fold).h5')\n",
        "\n",
        "    return Accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAOiWQMzuYZC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def result_model(dataset, num_classes, model_dir, graph_dir, jaffe_dir_path = None,\n",
        "                 ck_dir_path = None,cv = 'k_fold', batch_size = 16, iterations = 120,\n",
        "                 beta = 0.9, n_splits = 10):\n",
        "    '''\n",
        "    Returns : the results of trained model with given arguments\n",
        "    Arguments :\n",
        "    dataset -- A string representing dataset('ck+' or 'jaffe' or 'combined')\n",
        "    num_classes -- No. of emotions the model to be trained upon'''\n",
        "    #model_dir -- path to the directory/folder to save trained model header file\n",
        "                 #(eg : C:\\users\\folder\\)\n",
        "    #graph_dir -- path to the directory/folder to save various graphs\n",
        "    #dataset_dir_path -- path to the saved dataset directory\n",
        "    #cv -- string representing k-fold or 1-fold cross validation('k_fold' or 'one_fold')\n",
        "    #beta -- momentum variable\n",
        "    #n_splits -- representing k in k_fold\n",
        "\n",
        "    if dataset == 'combined':\n",
        "        num_classes = 6\n",
        "    if jaffe_dir_path == None and ck_dir_path == None:\n",
        "        print('Dataset path is needed!!')\n",
        "        return None,None\n",
        "    else:\n",
        "        X_tmp, Y_tmp = load_and_preprocess_data(dataset, num_classes, jaffe_dir_path, ck_dir_path)\n",
        "        if cv == 'one_fold':\n",
        "            Accuracy = one_fold_cv(X_tmp, Y_tmp, dataset, num_classes, model_dir, graph_dir,\n",
        "                        batch_size, iterations, beta)\n",
        "            return Accuracy,None\n",
        "        elif cv == 'k_fold':\n",
        "            Mean_Accuracy, standard_deviation = k_fold_cv(X_tmp, Y_tmp, dataset, num_classes, model_dir,\n",
        "                                                          graph_dir,n_splits,batch_size,iterations,beta)\n",
        "            print('Mean Accuracy : %0.2f' %(Mean_Accuracy))\n",
        "            print('standard deviation : %0.2f' %(standard_deviation))\n",
        "            return Mean_Accuracy, standard_deviation\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Mbb7K8PQ4NM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Mean_Accuracy, standard_deviation = result_model('combined',num_classes = 6, model_dir = 'drive/My Drive/ER project/',\n",
        "                                                 graph_dir = 'drive/My Drive/ER project/',cv = 'one_fold',\n",
        "                                                 jaffe_dir_path = 'drive/My Drive/Dataset Images/Jaffe Images/',\n",
        "                                                 ck_dir_path = 'drive/My Drive/Dataset Images/CK+48/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oc4v3q-b8QEg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}